{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer, InputSpec, Conv2D, Conv2DTranspose, Activation, Reshape, LayerNormalization, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, UpSampling2D, Dropout, Concatenate, Add, Dense, Multiply, LeakyReLU, Flatten, AveragePooling2D, Multiply\n",
    "from tensorflow.keras import initializers, regularizers, constraints, Model, Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import RandomNormal\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical, plot_model\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'PGGAN'\n",
    "DATA_BASE_DIR = '../dataset/CelebAMask-HQ/CelebA-HQ-img' # Modify this to your dataset path.\n",
    "OUTPUT_PATH = 'outputs'\n",
    "MODEL_PATH = 'models'\n",
    "TRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", 'train_data') # Sets up a log directory.\n",
    "if not os.path.exists(OUTPUT_PATH):\n",
    "    os.makedirs(OUTPUT_PATH)\n",
    "    \n",
    "batch_size = 16\n",
    "image_size = 4 # Start from 4 * 4\n",
    "NOISE_DIM = 512\n",
    "LAMBDA = 10\n",
    "\n",
    "EPOCHs = 320\n",
    "CURRENT_EPOCH = 1 # Epoch start from 1. If resume training, set this to the previous model saving epoch.\n",
    "\n",
    "total_data_number = len(os.listdir(DATA_BASE_DIR))\n",
    "\n",
    "# To reduce the training time, this number is lower than the original paper,\n",
    "# thus the output quality would be worse than the original.\n",
    "switch_res_every_n_epoch = 40\n",
    "#switch_res_every_n_epoch = math.ceil(800000 / total_data_number)\n",
    "\n",
    "SAVE_EVERY_N_EPOCH = 5 # Save checkpoint at every n epoch\n",
    "\n",
    "LR = 1e-3\n",
    "BETA_1 = 0.\n",
    "BETA_2 = 0.99\n",
    "EPSILON = 1e-8\n",
    "# Decay learning rate\n",
    "MIN_LR = 0.000001\n",
    "DECAY_FACTOR=1.00004\n",
    "\n",
    "# Creates a file writer for the log directory.\n",
    "file_writer = tf.summary.create_file_writer(TRAIN_LOGDIR)\n",
    "#test_file_writer = tf.summary.create_file_writer(TEST_LOGDIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ds = tf.data.Dataset.list_files(DATA_BASE_DIR + '/*')\n",
    "\n",
    "for f in list_ds.take(5):\n",
    "    print(f.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "    '''\n",
    "        normalizing the images to [-1, 1]\n",
    "    '''\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = (image - 127.5) / 127.5\n",
    "    return image\n",
    "\n",
    "def augmentation(image):\n",
    "    '''\n",
    "        Perform some augmentation\n",
    "    '''\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image\n",
    "\n",
    "def preprocess_image(file_path, target_size=512):\n",
    "    images = tf.io.read_file(file_path)\n",
    "    # convert the compressed string to a 3D uint8 tensor\n",
    "    images = tf.image.decode_jpeg(images, channels=3)\n",
    "    images = tf.image.resize(images, (target_size, target_size),\n",
    "                           method='nearest', antialias=True)\n",
    "    images = augmentation(images)\n",
    "    images = normalize(images)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_function = partial(preprocess_image, target_size=image_size)\n",
    "train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = next(iter(train_data))\n",
    "plt.title('Sample')\n",
    "plt.imshow(np.clip(sample_img[0] * 0.5 + 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EqualizeLearningRate(tf.keras.layers.Wrapper):\n",
    "    \"\"\"\n",
    "    Reference from WeightNormalization implementation of TF Addons\n",
    "    EqualizeLearningRate wrapper works for keras CNN and Dense (RNN not tested).\n",
    "    ```python\n",
    "      net = EqualizeLearningRate(\n",
    "          tf.keras.layers.Conv2D(2, 2, activation='relu'),\n",
    "          input_shape=(32, 32, 3),\n",
    "          data_init=True)(x)\n",
    "      net = EqualizeLearningRate(\n",
    "          tf.keras.layers.Conv2D(16, 5, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = EqualizeLearningRate(\n",
    "          tf.keras.layers.Dense(120, activation='relu'),\n",
    "          data_init=True)(net)\n",
    "      net = EqualizeLearningRate(\n",
    "          tf.keras.layers.Dense(n_classes),\n",
    "          data_init=True)(net)\n",
    "    ```\n",
    "    Arguments:\n",
    "      layer: a layer instance.\n",
    "    Raises:\n",
    "      ValueError: If `Layer` does not contain a `kernel` of weights\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, layer, **kwargs):\n",
    "        super(EqualizeLearningRate, self).__init__(layer, **kwargs)\n",
    "        self._track_trackable(layer, name='layer')\n",
    "        self.is_rnn = isinstance(self.layer, tf.keras.layers.RNN)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        \"\"\"Build `Layer`\"\"\"\n",
    "        input_shape = tf.TensorShape(input_shape)\n",
    "        self.input_spec = tf.keras.layers.InputSpec(\n",
    "            shape=[None] + input_shape[1:])\n",
    "\n",
    "        if not self.layer.built:\n",
    "            self.layer.build(input_shape)\n",
    "\n",
    "        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n",
    "\n",
    "        if not hasattr(kernel_layer, 'kernel'):\n",
    "            raise ValueError('`EqualizeLearningRate` must wrap a layer that'\n",
    "                             ' contains a `kernel` for weights')\n",
    "\n",
    "        if self.is_rnn:\n",
    "            kernel = kernel_layer.recurrent_kernel\n",
    "        else:\n",
    "            kernel = kernel_layer.kernel\n",
    "\n",
    "        # He constant\n",
    "        self.fan_in, self.fan_out= self._compute_fans(kernel.shape)\n",
    "        self.he_constant = tf.Variable(1.0 / np.sqrt(self.fan_in), dtype=tf.float32, trainable=False)\n",
    "\n",
    "        self.v = kernel\n",
    "        self.built = True\n",
    "    \n",
    "    def call(self, inputs, training=True):\n",
    "        \"\"\"Call `Layer`\"\"\"\n",
    "\n",
    "        with tf.name_scope('compute_weights'):\n",
    "            # Multiply the kernel with the he constant.\n",
    "            kernel = tf.identity(self.v * self.he_constant)\n",
    "            \n",
    "            if self.is_rnn:\n",
    "                print(self.is_rnn)\n",
    "                self.layer.cell.recurrent_kernel = kernel\n",
    "                update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n",
    "            else:\n",
    "                self.layer.kernel = kernel\n",
    "                update_kernel = tf.identity(self.layer.kernel)\n",
    "\n",
    "            # Ensure we calculate result after updating kernel.\n",
    "            with tf.control_dependencies([update_kernel]):\n",
    "                outputs = self.layer(inputs)\n",
    "                return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tf.TensorShape(\n",
    "            self.layer.compute_output_shape(input_shape).as_list())\n",
    "    \n",
    "    def _compute_fans(self, shape, data_format='channels_last'):\n",
    "        \"\"\"\n",
    "        From Official Keras implementation\n",
    "        Computes the number of input and output units for a weight shape.\n",
    "        # Arguments\n",
    "            shape: Integer shape tuple.\n",
    "            data_format: Image data format to use for convolution kernels.\n",
    "                Note that all kernels in Keras are standardized on the\n",
    "                `channels_last` ordering (even when inputs are set\n",
    "                to `channels_first`).\n",
    "        # Returns\n",
    "            A tuple of scalars, `(fan_in, fan_out)`.\n",
    "        # Raises\n",
    "            ValueError: in case of invalid `data_format` argument.\n",
    "        \"\"\"\n",
    "        if len(shape) == 2:\n",
    "            fan_in = shape[0]\n",
    "            fan_out = shape[1]\n",
    "        elif len(shape) in {3, 4, 5}:\n",
    "            # Assuming convolution kernels (1D, 2D or 3D).\n",
    "            # TH kernel shape: (depth, input_depth, ...)\n",
    "            # TF kernel shape: (..., input_depth, depth)\n",
    "            if data_format == 'channels_first':\n",
    "                receptive_field_size = np.prod(shape[2:])\n",
    "                fan_in = shape[1] * receptive_field_size\n",
    "                fan_out = shape[0] * receptive_field_size\n",
    "            elif data_format == 'channels_last':\n",
    "                receptive_field_size = np.prod(shape[:-2])\n",
    "                fan_in = shape[-2] * receptive_field_size\n",
    "                fan_out = shape[-1] * receptive_field_size\n",
    "            else:\n",
    "                raise ValueError('Invalid data_format: ' + data_format)\n",
    "        else:\n",
    "            # No specific assumptions.\n",
    "            fan_in = np.sqrt(np.prod(shape))\n",
    "            fan_out = np.sqrt(np.prod(shape))\n",
    "        return fan_in, fan_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_initializer = RandomNormal(mean=0.0, stddev=1.0)\n",
    "kernel_initializer = 'he_normal'\n",
    "\n",
    "class PixelNormalization(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      epsilon: a float-point number, the default is 1e-8\n",
    "    \"\"\"\n",
    "    def __init__(self, epsilon=1e-8):\n",
    "        super(PixelNormalization, self).__init__()\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return inputs / tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape\n",
    "\n",
    "class MinibatchSTDDEV(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Reference from official pggan implementation\n",
    "    https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py\n",
    "    \n",
    "    Arguments:\n",
    "      group_size: a integer number, minibatch must be divisible by (or smaller than) group_size.\n",
    "    \"\"\"\n",
    "    def __init__(self, group_size=4):\n",
    "        super(MinibatchSTDDEV, self).__init__()\n",
    "        self.group_size = group_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        group_size = tf.minimum(self.group_size, tf.shape(inputs)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
    "        s = inputs.shape                                             # [NHWC]  Input shape.\n",
    "        y = tf.reshape(inputs, [group_size, -1, s[1], s[2], s[3]])   # [GMHWC] Split minibatch into M groups of size G.\n",
    "        y = tf.cast(y, tf.float32)                              # [GMHWC] Cast to FP32.\n",
    "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMHWC] Subtract mean over group.\n",
    "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MHWC]  Calc variance over group.\n",
    "        y = tf.sqrt(y + 1e-8)                                   # [MHWC]  Calc stddev over group.\n",
    "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
    "        y = tf.cast(y, inputs.dtype)                                 # [M111]  Cast back to original data type.\n",
    "        y = tf.tile(y, [group_size, s[1], s[2], 1])             # [NHW1]  Replicate over group and pixels.\n",
    "        return tf.concat([inputs, y], axis=-1)                        # [NHWC]  Append as new fmap.\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1], input_shape[2], input_shape[3] + 1)\n",
    "\n",
    "\n",
    "def upsample_block(x, filters1, filters2, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
    "    '''\n",
    "        Upsampling + 2 Convolution-Activation\n",
    "    '''\n",
    "    upsample = UpSampling2D(size=2, interpolation='nearest')(x)\n",
    "    upsample_x = EqualizeLearningRate(Conv2D(filters1, kernel_size, strides, padding=padding,\n",
    "                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(upsample)\n",
    "    x = PixelNormalization()(upsample_x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = EqualizeLearningRate(Conv2D(filters2, kernel_size, strides, padding=padding,\n",
    "                                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = Activation(activation)(x)\n",
    "    return x, upsample\n",
    "\n",
    "def downsample_block(x, filters1, filters2, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
    "    '''\n",
    "        2 Convolution-Activation + Downsampling\n",
    "    '''\n",
    "    x = EqualizeLearningRate(Conv2D(filters1, kernel_size, strides, padding=padding,\n",
    "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    x = EqualizeLearningRate(Conv2D(filters2, kernel_size, strides, padding=padding,\n",
    "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
    "    x = Activation(activation)(x)\n",
    "    downsample = AveragePooling2D(pool_size=2)(x)\n",
    "\n",
    "    return downsample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I used tanh in the G output layers, I didn't test not using activation(linear in the paper) in the output layer, maybe using linear is better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_activation = tf.keras.activations.linear\n",
    "output_activation = tf.keras.activations.tanh\n",
    "\n",
    "def generator_input_block(x):\n",
    "    '''\n",
    "        Generator input block\n",
    "    '''\n",
    "    x = EqualizeLearningRate(Dense(4*4*512, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_dense')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Reshape((4, 4, 512))(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
    "                                          kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_conv2d')(x)\n",
    "    x = PixelNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    return x\n",
    "\n",
    "def build_4x4_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        4 * 4 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    # Not used in 4 * 4, put it here in order to keep the input here same as the other models\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
    "    \n",
    "    rgb_out = to_rgb(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=rgb_out)\n",
    "    return model\n",
    "\n",
    "def build_8x8_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        8 * 8 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    \n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    \n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
    "\n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_16x16_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        16 * 16 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
    "\n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_32x32_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        32 * 32 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
    "\n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_64x64_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        64 * 64 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=512, filters2=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
    "    \n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_128x128_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        128 * 128 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=256, filters2=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
    "    \n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_256x256_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        256 * 256 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    x, _ = upsample_block(x, filters1=256, filters2=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=128, filters2=64, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
    "    \n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model\n",
    "\n",
    "def build_512x512_generator(noise_dim=NOISE_DIM):\n",
    "    '''\n",
    "        512 * 512 Generator\n",
    "    '''\n",
    "    # Initial block\n",
    "    inputs = Input(noise_dim)\n",
    "    x = generator_input_block(inputs)\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
    "    x, _ = upsample_block(x, filters1=512, filters2=256, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
    "    x, _ = upsample_block(x, filters1=256, filters2=128, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
    "    x, _ = upsample_block(x, filters1=128, filters2=64, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    x, up_x = upsample_block(x, filters1=64, filters2=32, kernel_size=3, strides=1,\n",
    "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(512, 512))\n",
    "    \n",
    "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
    "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
    "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(512, 512))\n",
    "    \n",
    "    l_x = previous_to_rgb(up_x)\n",
    "    r_x = to_rgb(x)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    combined = Add()([l_x, r_x])\n",
    "    \n",
    "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_block(x):\n",
    "    '''\n",
    "        Discriminator output block\n",
    "    '''\n",
    "    x = MinibatchSTDDEV()(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
    "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_1')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = EqualizeLearningRate(Conv2D(512, 4, strides=1, padding='valid',\n",
    "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_2')(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Flatten()(x)\n",
    "    x = EqualizeLearningRate(Dense(1, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_dense')(x)\n",
    "    return x\n",
    "\n",
    "def build_4x4_discriminator():\n",
    "    '''\n",
    "        4 * 4 Discriminator\n",
    "    '''\n",
    "    inputs = Input((4,4,3))\n",
    "    # Not used in 4 * 4\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    # From RGB\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
    "    x = from_rgb(inputs)\n",
    "    x = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='conv2d_up_channel')(x)\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_8x8_discriminator():\n",
    "    '''\n",
    "        8 * 8 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((8,8,3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable block\n",
    "    ########################\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_16x16_discriminator():\n",
    "    '''\n",
    "        16 * 16 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((16, 16, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_32x32_discriminator():\n",
    "    '''\n",
    "        32 * 32 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((32, 32, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_64x64_discriminator():\n",
    "    '''\n",
    "        64 * 64 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 512\n",
    "    inputs = Input((64, 64, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    \n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=256, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_128x128_discriminator():\n",
    "    '''\n",
    "        128 * 128 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 256\n",
    "    inputs = Input((128, 128, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "   \n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=128, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_256x256_discriminator():\n",
    "    '''\n",
    "        256 * 256 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 128\n",
    "    inputs = Input((256, 256, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=64, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model\n",
    "\n",
    "def build_512x512_discriminator():\n",
    "    '''\n",
    "        512 * 512 Discriminator\n",
    "    '''\n",
    "    fade_in_channel = 64\n",
    "    inputs = Input((512, 512, 3))\n",
    "    alpha = Input((1), name='input_alpha')\n",
    "    downsample = AveragePooling2D(pool_size=2)\n",
    "    \n",
    "    ########################\n",
    "    # Left branch in the paper\n",
    "    ########################\n",
    "    previous_from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
    "    l_x = previous_from_rgb(downsample(inputs))\n",
    "    l_x = Multiply()([1 - alpha, l_x])\n",
    "    ########################\n",
    "    # Right branch in the paper\n",
    "    ########################\n",
    "    from_rgb = EqualizeLearningRate(Conv2D(32, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
    "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(512, 512))\n",
    "    r_x = from_rgb(inputs)\n",
    "    ########################\n",
    "    # Fade in block\n",
    "    ########################\n",
    "    r_x = downsample_block(r_x, filters1=32, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(512,512))\n",
    "    r_x = Multiply()([alpha, r_x])\n",
    "    x = Add()([l_x, r_x])\n",
    "    ########################\n",
    "    # Stable blocks\n",
    "    ########################\n",
    "    x = downsample_block(x, filters1=64, filters2=128, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
    "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
    "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
    "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
    "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
    "    x = discriminator_block(x)\n",
    "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(target_resolution):\n",
    "    '''\n",
    "        Helper function to build models\n",
    "    '''\n",
    "    generator = None\n",
    "    discriminator = None\n",
    "    if target_resolution == 4:\n",
    "        generator = build_4x4_generator()\n",
    "        discriminator = build_4x4_discriminator()\n",
    "    elif target_resolution == 8:\n",
    "        generator = build_8x8_generator()\n",
    "        discriminator = build_8x8_discriminator()\n",
    "    elif target_resolution == 16:\n",
    "        generator = build_16x16_generator()\n",
    "        discriminator = build_16x16_discriminator()\n",
    "    elif target_resolution == 32:\n",
    "        generator = build_32x32_generator()\n",
    "        discriminator = build_32x32_discriminator()\n",
    "    elif target_resolution == 64:\n",
    "        generator = build_64x64_generator()\n",
    "        discriminator = build_64x64_discriminator()\n",
    "    elif target_resolution == 128:\n",
    "        generator = build_128x128_generator()\n",
    "        discriminator = build_128x128_discriminator()\n",
    "    elif target_resolution == 256:\n",
    "        generator = build_256x256_generator()\n",
    "        discriminator = build_256x256_discriminator()\n",
    "    elif target_resolution == 512:\n",
    "        generator = build_512x512_generator()\n",
    "        discriminator = build_512x512_discriminator()\n",
    "    else:\n",
    "        print(\"target resolution models are not defined yet\")\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "generator, discriminator = model_builder(image_size)\n",
    "generator.summary()\n",
    "plot_model(generator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "discriminator.summary()\n",
    "plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "G_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
    "\n",
    "def learning_rate_decay(current_lr, decay_factor=DECAY_FACTOR):\n",
    "    new_lr = max(current_lr / decay_factor, MIN_LR)\n",
    "    return new_lr\n",
    "\n",
    "def set_learning_rate(new_lr, D_optimizer, G_optimizer):\n",
    "    '''\n",
    "        Set new learning rate to optimizers\n",
    "    '''\n",
    "    K.set_value(D_optimizer.lr, new_lr)\n",
    "    K.set_value(G_optimizer.lr, new_lr)\n",
    "    \n",
    "def calculate_batch_size(image_size):\n",
    "    if image_size < 64:\n",
    "        return 16\n",
    "    elif image_size < 128:\n",
    "        return 12\n",
    "    elif image_size == 128:\n",
    "        return 8\n",
    "    elif image_size == 256:\n",
    "        return 4\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, test_input, figure_size=(12,6), subplot=(3,6), save=True, is_flatten=False):\n",
    "    # Test input is a list include noise and label\n",
    "    predictions = model.predict(test_input)\n",
    "    fig = plt.figure(figsize=figure_size)\n",
    "    for i in range(predictions.shape[0]):\n",
    "        axs = plt.subplot(subplot[0], subplot[1], i+1)\n",
    "        plt.imshow(predictions[i] * 0.5 + 0.5)\n",
    "        plt.axis('off')\n",
    "    if save:\n",
    "        plt.savefig(os.path.join(OUTPUT_PATH, '{}x{}_image_at_epoch_{:04d}.png'.format(predictions.shape[1], predictions.shape[2], epoch)))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples_to_generate = 9\n",
    "\n",
    "# We will reuse this seed overtime (so it's easier)\n",
    "# to visualize progress in the animated GIF)\n",
    "sample_noise = tf.random.normal([num_examples_to_generate, NOISE_DIM], seed=0)\n",
    "sample_alpha = np.repeat(1, num_examples_to_generate).reshape(num_examples_to_generate, 1).astype(np.float32)\n",
    "generate_and_save_images(generator, 0, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=False, is_flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_noise = np.load('sample_noise.npy')\n",
    "sample_noise = tf.convert_to_tensor(np_noise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training steps\n",
    "\n",
    "Adding tf function decorator in training functions can speed up training, but it cannot work with auto growing network. Which means you have to re-run the notebook, adjust the hyper parameters and set new current epoch to start with the higher resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def WGAN_GP_train_d_step(generator, discriminator, real_image, alpha, batch_size, step):\n",
    "    '''\n",
    "        One training step\n",
    "        \n",
    "        Reference: https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "    '''\n",
    "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
    "    epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n",
    "    ###################################\n",
    "    # Train D\n",
    "    ###################################\n",
    "    with tf.GradientTape(persistent=True) as d_tape:\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            fake_image = generator([noise, alpha], training=True)\n",
    "            fake_image_mixed = epsilon * tf.dtypes.cast(real_image, tf.float32) + ((1 - epsilon) * fake_image)\n",
    "            fake_mixed_pred = discriminator([fake_image_mixed, alpha], training=True)\n",
    "            \n",
    "        # Compute gradient penalty\n",
    "        grads = gp_tape.gradient(fake_mixed_pred, fake_image_mixed)\n",
    "        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gradient_penalty = tf.reduce_mean(tf.square(grad_norms - 1))\n",
    "        \n",
    "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
    "        real_pred = discriminator([real_image, alpha], training=True)\n",
    "        \n",
    "        D_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n",
    "    # Calculate the gradients for discriminator\n",
    "    D_gradients = d_tape.gradient(D_loss,\n",
    "                                            discriminator.trainable_variables)\n",
    "    # Apply the gradients to the optimizer\n",
    "    D_optimizer.apply_gradients(zip(D_gradients,\n",
    "                                                discriminator.trainable_variables))\n",
    "    # Write loss values to tensorboard\n",
    "    if step % 10 == 0:\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('D_loss', tf.reduce_mean(D_loss), step=step)\n",
    "            \n",
    "@tf.function\n",
    "def WGAN_GP_train_g_step(generator, discriminator, alpha, batch_size, step):\n",
    "    '''\n",
    "        One training step\n",
    "        \n",
    "        Reference: https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "    '''\n",
    "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
    "    ###################################\n",
    "    # Train G\n",
    "    ###################################\n",
    "    with tf.GradientTape() as g_tape:\n",
    "        fake_image = generator([noise, alpha], training=True)\n",
    "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
    "        G_loss = -tf.reduce_mean(fake_pred)\n",
    "    # Calculate the gradients for discriminator\n",
    "    G_gradients = g_tape.gradient(G_loss,\n",
    "                                            generator.trainable_variables)\n",
    "    # Apply the gradients to the optimizer\n",
    "    G_optimizer.apply_gradients(zip(G_gradients,\n",
    "                                                generator.trainable_variables))\n",
    "    # Write loss values to tensorboard\n",
    "    if step % 10 == 0:\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.scalar('G_loss', G_loss, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.ceil(total_data_number / batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(MODEL_PATH):\n",
    "    os.makedirs(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load previous resolution model\n",
    "if image_size > 4:\n",
    "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
    "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
    "        print(\"generator loaded\")\n",
    "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
    "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
    "        print(\"discriminator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To resume training, comment it if not using.\n",
    "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size)))):\n",
    "    generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
    "    print(\"generator loaded\")\n",
    "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size)))):\n",
    "    discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
    "    print(\"discriminator loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "current_learning_rate = LR\n",
    "training_steps = math.ceil(total_data_number / batch_size)\n",
    "# Fade in half of switch_res_every_n_epoch epoch, and stablize another half\n",
    "alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
    "alpha = min(1., (CURRENT_EPOCH - 1) % switch_res_every_n_epoch * training_steps *  alpha_increment)\n",
    "\n",
    "for epoch in range(CURRENT_EPOCH, EPOCHs + 1):\n",
    "    start = time.time()\n",
    "    print('Start of epoch %d' % (epoch,))\n",
    "    print('Current alpha: %f' % (alpha,))\n",
    "    print('Current resolution: {} * {}'.format(image_size, image_size))\n",
    "    # Using learning rate decay\n",
    "#     current_learning_rate = learning_rate_decay(current_learning_rate)\n",
    "#     print('current_learning_rate %f' % (current_learning_rate,))\n",
    "#     set_learning_rate(current_learning_rate) \n",
    "    \n",
    "    for step, (image) in enumerate(train_data):\n",
    "        current_batch_size = image.shape[0]\n",
    "        alpha_tensor = tf.constant(np.repeat(alpha, current_batch_size).reshape(current_batch_size, 1), dtype=tf.float32)\n",
    "        # Train step\n",
    "        \n",
    "        WGAN_GP_train_d_step(generator, discriminator, image, alpha_tensor,\n",
    "                             batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
    "        WGAN_GP_train_g_step(generator, discriminator, alpha_tensor,\n",
    "                             batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
    "        \n",
    "        \n",
    "        # update alpha\n",
    "        alpha = min(1., alpha + alpha_increment)\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            print ('.', end='')\n",
    "    \n",
    "    # Clear jupyter notebook cell output\n",
    "    clear_output(wait=True)\n",
    "    # Using a consistent image (sample_X) so that the progress of the model is clearly visible.\n",
    "    generate_and_save_images(generator, epoch, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=True, is_flatten=False)\n",
    "    \n",
    "    if epoch % SAVE_EVERY_N_EPOCH == 0:\n",
    "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
    "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
    "        print ('Saving model for epoch {}'.format(epoch))\n",
    "    \n",
    "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
    "                                                      time.time()-start))\n",
    "    \n",
    "    \n",
    "    # Train next resolution\n",
    "    if epoch % switch_res_every_n_epoch == 0:\n",
    "        print('saving {} * {} model'.format(image_size, image_size))\n",
    "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
    "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
    "        # Reset alpha\n",
    "        alpha = 0\n",
    "        previous_image_size = int(image_size)\n",
    "        image_size = int(image_size * 2)\n",
    "        if image_size > 512:\n",
    "            print('Resolution reach 512x512, finish training')\n",
    "            break\n",
    "        print('creating {} * {} model'.format(image_size, image_size))\n",
    "        generator, discriminator = model_builder(image_size)\n",
    "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
    "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
    "        \n",
    "        print('Making {} * {} dataset'.format(image_size, image_size))\n",
    "        batch_size = calculate_batch_size(image_size)\n",
    "        preprocess_function = partial(preprocess_image, target_size=image_size)\n",
    "        train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)\n",
    "        training_steps = math.ceil(total_data_number / batch_size)\n",
    "        alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
    "        print('start training {} * {} model'.format(image_size, image_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the resolution to the number you want to test\n",
    "resolution = 4\n",
    "generator, discriminator = model_builder(resolution)\n",
    "generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(resolution, resolution)))\n",
    "discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(resolution, resolution)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_noise = tf.random.normal([9, NOISE_DIM])\n",
    "test_alpha = tf.ones([9, 1])\n",
    "#generate_and_save_images(generator, 0, [test_noise], figure_size=(12,6), subplot=(3,6), save=False, is_flatten=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = generator.predict([test_noise, test_alpha])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(images, fig):\n",
    "    \"\"\"Return a 3x3 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
    "    # Create a figure to contain the plot.\n",
    "    for i in range(9):\n",
    "        # Start next subplot.\n",
    "        axs = fig.add_subplot(3, 3, i + 1)\n",
    "        axs.set_xticks([])\n",
    "        axs.set_yticks([])\n",
    "        axs.imshow(np.clip(images[i] * 0.5 + 0.5, 0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the real images\n",
    "fig1 = plt.figure(figsize=(9,9))\n",
    "image_grid(sample_img.numpy(), fig1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fake images from the last epoch\n",
    "fig2 = plt.figure(figsize=(9,9))\n",
    "image_grid(prediction, fig2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
